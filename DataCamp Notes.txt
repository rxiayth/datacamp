DataCamp Notes

Intro to Python for Data Science
    Variables and Types --> type(var); str, int, bool, list 
        Everything is an objects 
    Type Conversion --> str(float)
    Lists --> l = [v1,v2,v3], l[-1], l[<inc>:<exc>]
        operations: l[:] = [], l1 +l2 , del(l[3])
        *Variables are saved by reference
        To deep clone --> y = list(x) or y = x[:]
        sorted(list, reverse=True)
        
    Functions --> max(fam), round(float,2), print(var), 
    help --> ?functionName
    Methods; functions belonging to an object
        list.index, list.count, list.append (know diff compared to '+' op)
        list.reverse
        str.capitalize(), str.replace(find,replace)
    Packages --> directory of scripts, each script is a module
        Numpy --> fast operation over numpy array 
            Assumption: array is of one type
            bmiArray > 23 -> npArray of dtype=bool
            bmi[bmi>23] --> npArray of elements passing condition

            npArray.shape --> #rows, #columns
            np_2d[:,1:3]
            np.mean( np_city[:,0]), np.median, np.correcoef, np.std
            np.sum, np.sort

            height = np.round( np.random.normal(1.75, .20, 5000), 2)
            np.logical_and(,), np.logical_or(,), np.logical_not(,)

            


Intermediate Python for Data Science 
    Matplotlib
        plt.plot( year, pop ), plt.scatter(x,y)
        plt.xscale('log')
        plt.show()

        histogram - distribution
            plt.hist(x,bins,range,normed,weights)
            plt.xlable('Year')
            plt.ylabel('Population')
            plt.title('World Population Projection)
            plt.yticks([0,2,4,6,8,10]. ['0', '2B', '4B', '6B', '8B', '10B'])
            plt.text(1550, 71, 'India')
            plt.grid(True)


    Dictionaries and Pandas
        dict = {k1:v1,k2:v2}; dict[key], dict.keys()
            pros: order don't matter, and has faster lookup

    Pandas - dataFrame
        df = pd.dataFrame(dict)
        df.index = [v1,v2,v3,v4]
        
        
        col access
            df[colName, colName2] --> type =  series          
            df[[colName, colName2]] --> type = dataFrame
        row Access
            df.loc[rowIndex] --> type = series
            df.loc[[rowIndex],[colIndex]] --> type = dataFrame ; can use : to select all
        others
            iloc

        df2 = pd.read_csv("path/to/df.csv", index_col=0)

        E.g., Select Countries w/  area > 8mil
            1) select area col --> isHuge = df['area']
            2) select DF meeting criteria --> dfIsHuge = df[isHuge]
            3) select countries --> dfIsHuge[0]

    Logic, Control Flow
        operators; <,>,==,>=,<=,!=, not, and, or
        if, elif, else

    Loops
        while cond: 
        for index, item in enumerate(list):
        for key, val in dict.items():
        for val in np.nditer(multiDimArray):
        for lab, row in pd.iterrows():

        Panda adding column
            OLD APPROACH: 
                for lab, row in pd.iterrows(): 
                    df.loc[lab, 'newCol'] = len(row['country'])
            NEW APPROACH:
                df['newCol'] = df['country'].apply(len)

    


DataScience Toolbox 1/2
    Functions; argument is value passed into parameter defined in header
        def square both(x=2, y): 
            """Return strings of value"""
            tuple = (x**2, y**2)
            return tuple

            tuples are immutable

        def sum(*args):
        def sum(**kwargs):    ; dictionary
            
    Scope
        local, enclosing fn, global, built-in
    Nested Function
        helper, closure,  (nonlocal)
    
    Lambda and error Handling
        map(lambda num: num**2, numsList)  #print(list(squareAll))
        filter(fn, list)
        raise ValueError("blah")
        try: 
        except <TypeError>:
            

DataScience Toolbox 2/2
    Iterator is an object with iter() method
        it = iter(word), next(it), next(it)
            print(*it)
    
        enumerate(list, start=1) --> tuple (k:v)

        zip(l1,l2) --> iterable in tuples
            print(*zipIterable)
            
        load data in chunks
            for chunk in pd.read_csv('data.csv', chunksize=1000):

        Note: tuple vs list; mutability
                
    List Comphrehension
        one liner loop --> new_nums = [num + 1 for num in nums]
                       --> pairs_2  = [(n1,n2) for n1 in range(0,2) for n2 in range(6,8)]
                    [ <condition> comprehension <condition>]
        dict commprehensions
            {k:v for num in rane(9)}

    Generator
        one liner loop --> use () instead of [];
        ** generator list not stored in memory; next()
        function uses yield instead of return


Data Import 1/2
    Text file
        fn = 'fn'
        file = open(fn, mode='r'); mode = w
        file.readline()
        file.close()

        context manager (no need to close)
            with open(fn, 'r') as file:
                print( file.read() )


    Flat file (table); .csv, .txt; not RDB
        dtype = numericals
            np.loadtxt(fn, delimiter=',', skiprows=1, usecols=[0,2], dtype=str)
        dtype = mixed   
            np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None)
        mixed types?
            data = pd.readcsv(fn,nrows=5,header=None, sep='\t', comment='#', na_values="Nothing")
            data.head(), data.values,


    Ohter file types; excel, MATLAB, SAS, Stat, HDF5
        Pickled file
            with open(fn, 'rb') as file: 
                data = pickle.load(file)
        Excel file  
            data = pd.ExcelFile(fn)
            print(data.sheet_names)
            df1 = data.parse('sheet_name')
        SAS: biz analytics and biostatistics
        STAT: academic social sciences research
        HDF: hierarchical data format --> big files
        MATLAB: mathematics and statistics
        RDB: linked tables 

    DB Engine
        Postgress, SQLite. SQLAlchemy, MySQL
        # Import packages
        from sqlalchemy import create_engine
        import pandas as pd

        engine = create_engine('sqlite:///Chinook.sqlite')
        
        query = ("SELECT * from Album")
        df = pd.read_sql_query( query, engine)

   


Data Import 2/2
    requests package supports restful API
    BeautifulSoup package enable extraction of data
    http://seaborn.pydata.org/
    
Data Cleaning
    https://campus.datacamp.com/courses/cleaning-data-in-python/exploring-your-data?ex=1
    Tidy Data   
        *there are format better for reporting, and format better for analysis
        col represent separate vars
        row represent individual observations
        observational unit form tables
    
    Melt; reduce columns
        pd.melt(frame=df, id_vars=["keepCol1","keepCol2"], values_vars=["treatment a", "treatment b"], var_name="treatments", value_name="reading")

    Pivot; reduce rows
        pivot, pivot_table (aggfunc=np.mean; when duplicate rows exists)
        airquality_pivot = airquality_melt.pivot_table(index=["keepCol1", "keepCol2"], columns="pivotColVar", values="pivotColValue")
        airquality_pivot.reset_index()

    Str.split('_')

    Combine Data
        pd.concat([weater1,weather2], axis=0, ignore_index=True)

    Globbing; pattern matching for file names
        files = glob.glob(*.csv), pd.concat(files)
    Merge; on id
        pd.merge(left=state_pop, right=state_cod, on=None, left_on="state", right_on="name")
        types: 1:1, 1:*, *:*

    Data Types
        df.dtypes
        df['sex'] = df['sex'].astype('category')
        pd.to_numeric( df['treamt a'], errors="coerce")
        Str manipulation
            float_pattern = re.compile(^\$\d*\.\d{2}$)

    Apply   
        tips['total_dollar_replace'] = tips['total_dollar'].apply(lambda x: x.replace('$', ''))
        tips['total_dollar_re'] = tips['total_dollar'].apply(lambda x: re.findall('\d+\.\d+', x)[0])

    Drop Duplicates
        PD.drop_duplicates()
    Fill Na
        PD[[col1,col2]].fillna('missing')

    Assert
        assert pd.notnull(ebola).all().all()

    df.types, df['col1'].to_numeric(), df['col1'].astype(str), df.count_values



Pandas 
    Dataframe (type frame)
    df.shape, df.columns, df.index
    df.iloc[r1:r2, c1:c2], df.loc[c1,c2]
    df.head(3), df.tail(3)
    df.info()
    Columns (type series)
    np_vals = df.values
    df_log10 = np.log10(df)

    users = pd.DataFrame( {'k': 'v', 'k2':'v2'})
    users = pd.DataFrame( dict(
                            list(
                                zip( 
                                    [list_lables], [list_columns]
                                )
                            )
                        ))
    broadcasting
        users['fees'] = 0 --> broadcasted to entire column
        users[ {k:v, k2:v2}] --> dict can be also used to broadcast


    pd.read_csv('fn', 
                index_col=0,
                header=None,
                names=[col, names],
                na_values={colName:['-1']}, 
                parse_dates[[0,1,2]])

    DF.to_csv(out_tsv, sep='\t')
    DF.plot(color='red', subplots=True), plt.title, plt.xlabel, plt.ylabel,
    DF.plot([col list])
    DF.plot(x='colName', y=['col1','col2'])
    DF.plot(kind='scatter', x='hp', y='mpg', s=sizes)
    DF.plot(kind="box", y=cols, subplots=True)  vs  DF[cols].plot(kind="box" subplots=True)

    ***data.plot(kind='hist'), data.plt.hist(), data.hist()
    DATA.plot(kind='hist', bins=10, range(4,8), cumulative=True, normed=True )
    mean = DF.mean(axis='columns')

    DataCamp Notes

Intro to Python for Data Science
    Variables and Types --> type(var); str, int, bool, list 
        Everything is an objects 
    Type Conversion --> str(float)
    Lists --> l = [v1,v2,v3], l[-1], l[<inc>:<exc>]
        operations: l[:] = [], l1 +l2 , del(l[3])
        *Variables are saved by reference
        To deep clone --> y = list(x) or y = x[:]
        sorted(list, reverse=True)
        
    Functions --> max(fam), round(float,2), print(var), 
    help --> ?functionName
    Methods; functions belonging to an object
        list.index, list.count, list.append (know diff compared to '+' op)
        list.reverse
        str.capitalize(), str.replace(find,replace)
    Packages --> directory of scripts, each script is a module
        Numpy --> fast operation over numpy array 
            Assumption: array is of one type
            bmiArray > 23 -> npArray of dtype=bool
            bmi[bmi>23] --> npArray of elements passing condition

            npArray.shape --> #rows, #columns
            np_2d[:,1:3]
            np.mean( np_city[:,0]), np.median, np.correcoef, np.std
            np.sum, np.sort

            height = np.round( np.random.normal(1.75, .20, 5000), 2)
            np.logical_and(,), np.logical_or(,), np.logical_not(,)

            


Intermediate Python for Data Science 
    Matplotlib
        plt.plot( year, pop ), plt.scatter(x,y)
        plt.xscale('log')
        plt.show()

        histogram - distribution
            plt.hist(x,bins,range,normed,weights)
            plt.xlable('Year')
            plt.ylabel('Population')
            plt.title('World Population Projection)
            plt.yticks([0,2,4,6,8,10]. ['0', '2B', '4B', '6B', '8B', '10B'])
            plt.text(1550, 71, 'India')
            plt.grid(True)


    Dictionaries and Pandas
        dict = {k1:v1,k2:v2}; dict[key], dict.keys()
            pros: order don't matter, and has faster lookup

    Pandas - dataFrame
        df = pd.dataFrame(dict)
        df.index = [v1,v2,v3,v4]
        
        
        col access
            df[colName, colName2] --> type =  series          
            df[[colName, colName2]] --> type = dataFrame
        row Access
            df.loc[rowIndex] --> type = series
            df.loc[[rowIndex],[colIndex]] --> type = dataFrame ; can use : to select all
        others
            iloc

        df2 = pd.read_csv("path/to/df.csv", index_col=0)

        E.g., Select Countries w/  area > 8mil
            1) select area col --> isHuge = df['area']
            2) select DF meeting criteria --> dfIsHuge = df[isHuge]
            3) select countries --> dfIsHuge[0]

    Logic, Control Flow
        operators; <,>,==,>=,<=,!=, not, and, or
        if, elif, else

    Loops
        while cond: 
        for index, item in enumerate(list):
        for key, val in dict.items():
        for val in np.nditer(multiDimArray):
        for lab, row in pd.iterrows():

        Panda adding column
            OLD APPROACH: 
                for lab, row in pd.iterrows(): 
                    df.loc[lab, 'newCol'] = len(row['country'])
            NEW APPROACH:
                df['newCol'] = df['country'].apply(len)

    


DataScience Toolbox 1/2
    Functions; argument is value passed into parameter defined in header
        def square both(x=2, y): 
            """Return strings of value"""
            tuple = (x**2, y**2)
            return tuple

            tuples are immutable

        def sum(*args):
        def sum(**kwargs):    ; dictionary
            
    Scope
        local, enclosing fn, global, built-in
    Nested Function
        helper, closure,  (nonlocal)
    
    Lambda and error Handling
        map(lambda num: num**2, numsList)  #print(list(squareAll))
        filter(fn, list)
        raise ValueError("blah")
        try: 
        except <TypeError>:
            

DataScience Toolbox 2/2
    Iterator is an object with iter() method
        it = iter(word), next(it), next(it)
            print(*it)
    
        enumerate(list, start=1) --> tuple (k:v)

        zip(l1,l2) --> iterable in tuples
            print(*zipIterable)
            
        load data in chunks
            for chunk in pd.read_csv('data.csv', chunksize=1000):

        Note: tuple vs list; mutability
                
    List Comphrehension
        one liner loop --> new_nums = [num + 1 for num in nums]
                       --> pairs_2  = [(n1,n2) for n1 in range(0,2) for n2 in range(6,8)]
                    [ <condition> comprehension <condition>]
        dict commprehensions
            {k:v for num in rane(9)}

    Generator
        one liner loop --> use () instead of [];
        ** generator list not stored in memory; next()
        function uses yield instead of return


Data Import 1/2
    Text file
        fn = 'fn'
        file = open(fn, mode='r'); mode = w
        file.readline()
        file.close()

        context manager (no need to close)
            with open(fn, 'r') as file:
                print( file.read() )


    Flat file (table); .csv, .txt; not RDB
        dtype = numericals
            np.loadtxt(fn, delimiter=',', skiprows=1, usecols=[0,2], dtype=str)
        dtype = mixed   
            np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None)
        mixed types?
            data = pd.readcsv(fn,nrows=5,header=None, sep='\t', comment='#', na_values="Nothing")
            data.head(), data.values,


    Ohter file types; excel, MATLAB, SAS, Stat, HDF5
        Pickled file
            with open(fn, 'rb') as file: 
                data = pickle.load(file)
        Excel file  
            data = pd.ExcelFile(fn)
            print(data.sheet_names)
            df1 = data.parse('sheet_name')
        SAS: biz analytics and biostatistics
        STAT: academic social sciences research
        HDF: hierarchical data format --> big files
        MATLAB: mathematics and statistics
        RDB: linked tables 

    DB Engine
        Postgress, SQLite. SQLAlchemy, MySQL
        # Import packages
        from sqlalchemy import create_engine
        import pandas as pd

        engine = create_engine('sqlite:///Chinook.sqlite')
        
        query = ("SELECT * from Album")
        df = pd.read_sql_query( query, engine)

   


Data Import 2/2
    requests package supports restful API
    BeautifulSoup package enable extraction of data
    http://seaborn.pydata.org/
    
Data Cleaning
    https://campus.datacamp.com/courses/cleaning-data-in-python/exploring-your-data?ex=1
    Tidy Data   
        *there are format better for reporting, and format better for analysis
        col represent separate vars
        row represent individual observations
        observational unit form tables
    
    Melt; reduce columns
        pd.melt(frame=df, id_vars=["keepCol1","keepCol2"], values_vars=["treatment a", "treatment b"], var_name="treatments", value_name="reading")

    Pivot; reduce rows
        pivot, pivot_table (aggfunc=np.mean; when duplicate rows exists)
        airquality_pivot = airquality_melt.pivot_table(index=["keepCol1", "keepCol2"], columns="pivotColVar", values="pivotColValue")
        airquality_pivot.reset_index()

    Str.split('_')

    Combine Data
        pd.concat([weater1,weather2], axis=0, ignore_index=True)

    Globbing; pattern matching for file names
        files = glob.glob(*.csv), pd.concat(files)
    Merge; on id
        pd.merge(left=state_pop, right=state_cod, on=None, left_on="state", right_on="name")
        types: 1:1, 1:*, *:*

    Data Types
        df.dtypes
        df['sex'] = df['sex'].astype('category')
        pd.to_numeric( df['treamt a'], errors="coerce")
        Str manipulation
            float_pattern = re.compile(^\$\d*\.\d{2}$)

    Apply   
        tips['total_dollar_replace'] = tips['total_dollar'].apply(lambda x: x.replace('$', ''))
        tips['total_dollar_re'] = tips['total_dollar'].apply(lambda x: re.findall('\d+\.\d+', x)[0])

    Drop Duplicates
        PD.drop_duplicates()
    Fill Na
        PD[[col1,col2]].fillna('missing')

    Assert
        assert pd.notnull(ebola).all().all()

    df.types, df['col1'].to_numeric(), df['col1'].astype(str), df.count_values



Pandas 
    Dataframe (type frame)
    df.shape, df.columns, df.index
    df.iloc[r1:r2, c1:c2], df.loc[c1,c2]
    df.head(3), df.tail(3)
    df.info()
    Columns (type series)
    np_vals = df.values
    df_log10 = np.log10(df)

    users = pd.DataFrame( {'k': 'v', 'k2':'v2'})
    users = pd.DataFrame( dict(
                            list(
                                zip( 
                                    [list_lables], [list_columns]
                                )
                            )
                        ))
    broadcasting
        users['fees'] = 0 --> broadcasted to entire column
        users[ {k:v, k2:v2}] --> dict can be also used to broadcast


    pd.read_csv('fn', 
                index_col=0,
                header=None,
                names=[col, names],
                na_values={colName:['-1']}, 
                parse_dates[[0,1,2]])

    DF.to_csv(out_tsv, sep='\t')
    DF.plot(color='red', subplots=True), plt.title, plt.xlabel, plt.ylabel,
    DF.plot([col list])
    DF.plot(x='colName', y=['col1','col2'])
    DF.plot(kind='scatter', x='hp', y='mpg', s=sizes)
    DF.plot(kind="box", y=cols, subplots=True)  vs  DF[cols].plot(kind="box" subplots=True)

    ***data.plot(kind='hist'), data.plt.hist(), data.hist()
    DATA.plot(kind='hist', bins=10, range(4,8), cumulative=True, normed=True )
    mean = DF.mean(axis='columns')
    df[df['origin'] == 'US'].count()

    DF = pd.read_csv(filename, index_col='Date', parse_dates=True)
    my_datetimes = pd.to_datetime(date_list, format=time_format)  
    ts4 = ts2.reindex(ts1.index, method='ffill')

    df1 = df['Temperature'].resample('6h').mean()
    daily_highs = august.resample('D').max()

    unsmoothed = df['Temperature']['2010-08-01':'2010-08-15']
    # Apply a rolling mean with a 24 hour window: smoothed
    smoothed = unsmoothed.rolling(window=24).mean()             #rolling always chained on
    # Create a new DataFrame with columns smoothed and unsmoothed: august
    august = pd.DataFrame({'smoothed':smoothed, 'unsmoothed':unsmoothed})



    https://campus.datacamp.com/courses/pandas-foundations/time-series-in-pandas?ex=11


Pandas Intermediate
Pandas Advanced
Database Intro
Data Visual - Python
Data Visual - Bokeh
Stat Intro
Stat Intermediate
Supervised Learning
Machine Learning
Unsupervised Learning
Deep Learning
Network Analysis
Pandas Intermediate
Pandas Advanced
Database Intro
Data Visual - Python
Data Visual - Bokeh
Stat Intro
Stat Intermediate
Supervised Learning
Machine Learning
Unsupervised Learning
Deep Learning
Network Analysis